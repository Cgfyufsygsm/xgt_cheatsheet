\documentclass[10pt,landscape,a4paper]{ctexart}
\usepackage[landscape,margin=0.3cm]{geometry}
\usepackage{multicol}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{titlesec}

% 紧凑设置
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.5pt plus 0.5pt}
\setlength{\columnsep}{0.2cm}
\setlist{nosep, leftmargin=*}
\linespread{1.08} % 减小行距，让公式不会被拉伸
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}
\setlength{\jot}{2pt} % 多行公式之间的间距

% 避免underfull警告
\tolerance=1000
\hbadness=10000
\raggedright % 左对齐，避免两端对齐造成的间距问题

% 数学公式紧凑设置
\AtBeginDocument{
  \thickmuskip=2mu plus 2mu % 粗间距
  \medmuskip=1mu plus 1mu minus 1mu % 中间距
  \thinmuskip=1mu % 细间距
  \setlength{\abovedisplayskip}{0pt plus 1pt}
  \setlength{\belowdisplayskip}{0pt plus 1pt}
  \setlength{\abovedisplayshortskip}{0pt}
  \setlength{\belowdisplayshortskip}{0pt}
}

% 紧凑的section标题格式
\titleformat{\section}{\normalfont\bfseries\normalsize}{\thesection}{0.5em}{}
\titlespacing*{\section}{0pt}{2pt plus 2pt minus 2pt}{1pt plus 1pt minus 1pt}

\titleformat{\subsection}{\normalfont\bfseries\small}{\thesubsection}{0.5em}{}
\titlespacing*{\subsection}{0pt}{1pt plus 1pt minus 1pt}{0.5pt plus 0.5pt minus 0.5pt}

\titleformat{\subsubsection}{\normalfont\bfseries\footnotesize}{\thesubsubsection}{0.5em}{}
\titlespacing*{\subsubsection}{0pt}{1pt plus 1pt minus 1pt}{0.5pt plus 0.5pt minus 0.5pt}

% 自定义命令
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}[1]{\text{Var}(#1)}
\newcommand{\cov}[1]{\text{Cov}(#1)}
\renewcommand{\d}{\mathrm{d}}

\pagestyle{empty}

\begin{document}

\begin{multicols*}{3}
\footnotesize % 使用较小字号

\section*{Misc}

\begin{itemize}
  \item $n\in \mathbb{N}^*$，$\forall k \in \mathbb{Z}, k \ge 4\log_2 n$，有 $k!\ge n^3$
  \item $\forall x\ge -1$，$\ln(1+x)\le x$；$\forall x\le 1/2$，$\ln(1-x)\ge -x-x^2$
  \item $\exp(x)\ge x+1$
  \item $\frac{\d}{\d t}\int_{a(t)}^{b(t)}f(x)\d x = f(b(t))b'(t) - f(a(t))a'(t)$
  \item Gram-Schmidt 正交化！
  \item \textbf{期望！}非负离散：$E(X) = \sum_{x=0}^{+\infty}P(X>x)$，连续 $E(X) = \int_0^{+\infty} P(X>x) \d x$，有分布函数 $F_X(x)$ 的话 $E(X) = \int_0^{+\infty} (1-F_X(x)) \d x$。如果分段函数要注意分段之前的部分 $1$ 也有积分贡献。
  \item 高斯积分：$\int_{-\infty}^{+\infty} e^{-tx^2}\d x = \sqrt{\pi/t}$
  \item 分部积分：$\int u \d v = uv - \int v \d u$
  \item 凑微分：$\int \d f(u) = \int f'(u) \d u$
  \item $\ln(1-x)=-\sum_{k\ge1} \frac{x^k}{k}$，$(1+x)^{\alpha} = \sum_{k\ge 0}\binom{\alpha}{k} x^k$
  \item $\int (ax+b)^n \d x = \frac{(ax+b)^{n+1}}{a(n+1)} + C$，也即 $\int x^n \d x = \frac{x^{n+1}}{n+1}+C$
  \item 伽马函数相关性质$$\Gamma(\alpha) = \int_0^{\infty} x^{\alpha-1} e^{-x} \d x$$ $\Gamma(\alpha + 1) = \alpha \cdot \Gamma(\alpha), \Gamma(n+1)=n!, \Gamma(n+\frac12) = \frac{(2n)!}{4^nn!}\sqrt{\pi}$
\end{itemize}


\section{概率论基本概念}

\textbf{概率公理化}：$S$ 为样本空间，$F$ 为 $S$ 的某些子集组成的事件域。如果定义在 $F$ 上的实值函数 $P$ 满足。1. $\forall A \in F,P(A)\ge 0$；2. $P(S)=1$；3. $\forall A_1,A_2,\ldots \in F$ 且两两互斥，有 $P\left(\bigcup_{i=1}^{\infty} A_i\right)=\sum_{i=1}^{\infty} P(A_i)$，则称 $P$ 为概率测度，$(S,F,P)$ 为概率空间。

\textbf{一般加法公式} $P(\bigcup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i) - \sum_{1\le i<j\le n}P(A_iA_j) + \cdots + (-1)^{n-1} P(A_1\cdots A_n)$

\textbf{贝叶斯公式} $\displaystyle P(A|B) = P(B|A) \cdot \frac{P(A)}{P(B)}$

相互独立比两两独立\textbf{更强}。

\textbf{Union Bound} $P\left(\bigcup A_i\right) \le \sum P(A_i)$，对 $A_i$ 无要求。

\textbf{证明存在可以通过证明其概率大于 0}

\section{离散随机变量}

$P(X\ge E(X))>0,P(X\le E(X))>0,\var{aX+b}=a^2\var{X}$

尾不等式/集中不等式：随机变量与期望的偏离。

\textbf{Markov}：对\textbf{非负} $X$，$E(X)>0,a>0$，有 $$P(X\ge a) \le \frac{E(X)}{a}\quad P(X\ge aE(X)) \le \frac{1}{a}$$

\textbf{Chebyshev}：对 $\sigma(X)>0,c>0$，$$\begin{aligned}P(|X-E(X)|\ge c\cdot \sigma(X))\le1 / c^2\\P(|X-E(X)|\ge a) \le {\var{X}} / {a^2}\end{aligned}$$

\vspace{-12pt}
\begin{center}
\vspace{-3pt}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{分布} & \textbf{分布列} $P(X=k)=$ & \textbf{期望} & \textbf{方差} \\
\hline
$B(n,p)$ & $\binom{n}{k}p^k(1-p)^{n-k}$ & $np$ & $np(1-p)$ \\
\hline
$G(p)$ & $p(1-p)^{k-1}$ & $\frac{1}{p}$ & $\frac{1-p}{p^2}$ \\
\hline
$\pi(\lambda)$ & ${\lambda^k e^{-\lambda}} / {k!}$ & $\lambda$ & $\lambda$ \\
\hline
$NB(r,p)$ & $\binom{k-1}{r-1}p^r(1-p)^{k-r}$ & $\frac{r}{p}$ & $\frac{r(1-p)}{p^2}$ \\
\hline
\end{tabular}
\vspace{-3pt}
\end{center}
\vspace{-3pt}

\textbf{二项分布}：$n$ 次独立重复伯努利实验成功次数。\textbf{几何分布}：第一次成功的试验次数。\textbf{负二项分布}：第 $r$ 次成功的次数。有 $G(p) = NB(1,p)$。\textbf{泊松分布}：单位时间内事件发生次数。几何分布具有\textbf{无记忆性}：$P(X>m+n | X>m)=P(X>n)$。$NB(r,p)$ 可以拆成 $r$ 个独立的 $G(p)$ 之和。

\section{连续随机变量}

\textbf{正态分布}：$X\sim N(\mu,\sigma^2)$，$f(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp(-\frac{(x-\mu)^2}{2\sigma^2})$，$E(X)=\mu,\var{X}=\sigma^2$。标准正态分布 $Z\sim N(0,1)$。

\textbf{指数分布}：$X\sim \text{Exp}(\lambda)$，$f(x)=\lambda e^{-\lambda x}$，$F(x) = 1-e^{-\lambda x}$，$E(X)=1/\lambda, \var{X}=1/\lambda^2$。具有无记忆性 $P(X>s+t|X>s) = P(X>t)$。理解为泊松分布假设下，第一次事件发生的时刻。

\textbf{伽马分布}：$X\sim \Gamma(\alpha,\lambda)$，理解为泊松假设下第 $\alpha$ 次的时刻。$$f(x) = \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1} e^{-\lambda x},x\ge0$$ $E(X)=\alpha / \lambda$，$\var{X}=\alpha / \lambda^2$。具有可加性：$\Gamma(\alpha_1+\alpha_2, \lambda) = \Gamma(\alpha_1, \lambda) + \Gamma(\alpha_2, \lambda)$。$\alpha=1$ 时即指数分布。$\alpha=n/2, \lambda=1/2$ 时即 $\chi^2(n)$ 分布。

\textbf{概率密度变换}：$Y=g(X)$，$g$ 单调且反函数 $h(y)$ 有连续导数，则 $f_Y(y) = f_X(h(y))\cdot |h'(y)|$。如果没法直接套这个公式的话可以从\textbf{分布函数}的定义出发进行变换。即先算 $P(Y\le y)$ 再求导。

\section{多维离散随机变量}

对于\textbf{独立}的 $X,Y$，有 $\var{X \pm Y} = \var{X} + \var{Y}$

\textbf{协方差}：$\cov{X,Y} = E((X-E(X))(Y-E(Y))) = E(XY) - E(X)E(Y)$。

$\var{X_1+\cdots+X_n} = \sum_i\sum_j \cov{X_i,X_j}$，$\var{X+Y}=\var{X}+\var{Y} + 2\cov{X,Y}$，$\cov{aX,bY} = ab\cdot \cov{X,Y}$，$\cov{X_1+X_2,Y}=\cov{X_1,Y}+\cov{X_2,Y}$，$\cov{X,Y}\le \sqrt{\var{X}\var{Y}}$

\textbf{条件期望}：$E(X|Y=y)=\sum_i x_i P(X=x_i|Y=y)$，是关于 $y$ 的函数。$E(X|Y)$ 是随机变量。

\textbf{重期望公式}：$E(E(X|Y)) = E(X)$

\section{多维连续随机变量}

\textbf{条件分布函数}：$F(x|y) = P(X\le x|Y=y) = \int_{-\infty}^x \frac{f(u,y)}{f_Y(y)} \d u$

\textbf{条件密度函数}： $f(x|y) = \frac{\partial}{\partial x} F(x|y) = \frac{f(x,y)}{f_Y(y)}$。

\textbf{二维高斯}：$X,Y\sim N(\mu_1,\mu_2, \sigma_1^2,\sigma_2^2,\rho)$，要求 $|\rho|<1$。$$\resizebox{\linewidth}{!}{$\displaystyle \frac{1}{2\pi \sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left[-\frac{1}{2(1-\rho^2)}\left(\frac{(x-\mu_1)^2}{\sigma_1^2}+\frac{(y-\mu_2)^2}{\sigma_2^2}-\frac{2\rho(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2}\right) \right]$}$$ 边际密度函数与 $\rho$ 无关，即 $X\sim N(\mu_1, \sigma_1^2), Y \sim N(\mu_2, \sigma_2^2)$。$\rho=0$ 时独立。协方差 $\cov{X,Y} = \rho\sigma_1\sigma_2$。给定条件 $Y=y$，$X$ 的条件分布服从 $N(\mu_1+\rho\frac{\sigma_1}{\sigma_2}(y-\mu_2),\sigma_1^2(1-\rho^2))$

\textbf{相关系数} $\text{Corr}(X,Y)=\cov{X,Y}/\sigma(X)\sigma(Y)$。相关系数/协方差大于 0 则正相关，小于 0 则负相关，等于 0 则不相关（但不一定独立）。相关系数等于 $\pm1$ 代表 $X,Y$ 呈严格线性关系。证明考虑标准化 $\tilde{X},\tilde{Y}$，然后通过 $\var{\tilde{X}-\tilde{Y}}=0$ 推导出 $P(\tilde{X}-\tilde{Y}=c)=1$。

\subsection{概率密度变换}

\textbf{卷积公式}：若 $X,Y$ 独立，$Z=X+Y$，则 $$f_Z(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(z-x) \d x = \int_{-\infty}^{\infty} f_X(z-y) f_Y(y) \d y$$

\textbf{min/max}：若 $X_1,\cdots,X_n$ 独立，则 $Y = \max\{X_i\}$ 的分布函数为 $F_Y(y) = \prod_{i=1}^n F_{X_i}(y)$，$Y=\min\{X_i\}$ 的分布函数为 $F_Y(y) = 1 - \prod_{i=1}^n (1 - F_{X_i}(y))$。

\textbf{换元}：$X,Y$ 的概率密度为 $f(x,y)$，函数 $u=u(x,y)$ 和 $v=v(x,y)$ 偏导连续且 $x=x(u,v),y=y(u,v)$ 为唯一反函数，则 $U=u(X,Y),V=v(X,Y)$ 的联合概率密度为 $$g(u,v) = f(x(u,v),y(u,v)) \cdot |J|, J = \left|\frac{\partial(x,y)}{\partial(u,v)}\right| = \begin{vmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{vmatrix}$$

% // TODO 添加例子

\subsection{线性代数}

\textbf{对角化}：$\boldsymbol{A} = \boldsymbol{P}\boldsymbol{\Lambda}\boldsymbol{P}^{-1}$。保行列式，平方的行列式和 trace 不变。实对称矩阵可对角化，不同特征值的特征向量正交。

\textbf{正定}：半正定矩阵 $\boldsymbol{A}$ 存在 $\boldsymbol{B} = \boldsymbol{A}^{1/2}$，$\boldsymbol{B}^\top \boldsymbol{B} = \boldsymbol{B}^2=\boldsymbol{A}$，且 $B$ 不唯一。对于正定矩阵，这样的 $\boldsymbol{B}$ 可逆，$(\boldsymbol{B}^{-1})^{2}=\boldsymbol{A}^{-1}$。

\textbf{协方差矩阵}：对于随机变量 $\boldsymbol{X} = (X_1,\cdots,X_n)$，$\cov{\boldsymbol{X}} = E((\boldsymbol{X}-E(\boldsymbol{X}))(\boldsymbol{X}-E(\boldsymbol{X}))^\top)$ 为协方差矩阵，有 $$\resizebox{\linewidth}{!}{$\cov{\boldsymbol{X}}=\begin{pmatrix}
  \var{X_1}&\cov{X_1,X_2}&\cdots&\cov{X_1,X_n}\\
  \cov{X_2,X_1}&\var{X_2}&\cdots&\cov{X_2,X_n}\\
  \vdots&\vdots&&\vdots\\
  \cov{X_n,X_1}&\cov{X_n,X_2}&\cdots&\var{X_n}
\end{pmatrix}$}$$ 其对称且半正定。

\textbf{高斯}：$n$ 维高斯的联合密度函数，$\boldsymbol{B}$ 为协方差矩阵：$$f(\boldsymbol{x}) = \frac{1}{(2\pi)^{\frac n2}(\det\boldsymbol{B})^{\frac12}}\exp\left(-\frac12\cdot (\boldsymbol{x}-\boldsymbol{\mu})^\top \boldsymbol{B}^{-1} (\boldsymbol{x} - \boldsymbol{\mu})\right)$$ 若 $\boldsymbol{X} \sim N(\boldsymbol{\mu},\boldsymbol{B})$，令 $\boldsymbol{Y} = \boldsymbol{AX} + \boldsymbol{b}$，且 $\boldsymbol{A}$ 行满秩，则 $Y\sim N(\boldsymbol{A \mu} + \boldsymbol{b}, \boldsymbol{AB}\boldsymbol{A}^\top)$

\subsection{结论}

\begin{itemize}
  \item 对于独立的 $X_i\sim N(\mu_i, \sigma^2_i)$，有 $$\sum a_i X_i \sim N(\sum a_i\mu_i, \sum a_i^2 \sigma_i^2)$$
  \item 对于单个 $X_i \sim N(0,1)$，有 $X_i^2 \sim \chi^2(1)=\Gamma(1/2,1/2)$，且 $\sum_{i=1}^n X_i^2 \sim \chi^2(n)=\Gamma(\frac{n}{2}, \frac{1}{2})$
\end{itemize}

\section{尾不等式、大数定律与中心极限定理}

\subsection{尾不等式}

尾不等式：$P(X\ge k)$ 的上界。集中不等式：$P(|X-E(X)|\ge k)$ 的上界。

\textbf{矩}：$E(X^n)$ 称为 $X$ 的 $n$ 阶矩，$E((X-E(X))^n)$ 称为 $X$ 的 $n$ 阶中心矩。切比雪夫不等式的本质是对二阶中心矩使用 Markov。

\textbf{矩生成函数}：$M_X(t) = E(e^{tX}) = \sum_{i\ge0}\frac{t^i}{i!}E(X^i)$。所以求 $k$ 阶矩可以求其封闭形式的 $k$ 阶导然后令 $t=0$。

\textbf{Chernoff Bound}：求 $k$ 阶中心矩然后用 Markov 得到的尾不等式通常较弱（没有真正用到 $n$ 重伯努利实验的独立性）。对于任意 $t>0$，有 $$P(X\ge a) = P(e^{tX} \ge e^{ta}) \le \frac{E(e^{tX})}{e^{ta}} = e^{-ta} M_X(t)$$ 对于任意 $t<0$ 有 $P(X\le a)\le M_X(t)\cdot e^{-t a} $ 通过调节 $t$ 可得到更紧的上界。一般而言是\textbf{求导}找最小值。但是 Chernoff Bound 不一定是最紧的。

\textbf{Hoeffding 引理}：若实数随机变量 $a\le X\le b$， 则对任意实数 $t$ 有 $$E(e^{t(X-E(X))}) \le \exp\left(\frac{t^2(b-a)^2}{8}\right)$$

\textbf{Chernoff-Hoeffding 不等式}：$X_1,\cdots,X_n$ 独立，且 $a_i\le X_i \le b_i$，令 $X = \sum_{i=1}^n X_i$，则对任意 $t>0$ 有 $$\begin{aligned}
P(X \ge E(X) + t) &\le \exp\left(\frac{-2t^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)\\
P(X \le E(X) - t) &\le \exp\left(\frac{-2t^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)
\end{aligned} $$ 对于 $a\le X_i\le b$ 的情况，分母就是 $n(b-a)^2$。

\subsection{尾不等式结论}

\textbf{矩生成函数的应用}：对于要算 $e^X$ 的期望，可以先算矩生成函数然后令 $t=1$。

\textbf{矩生成函数}：\begin{itemize}
  \item $X\sim B(n,p)$，$M_X (t) = (1-p+e^t p)^n$
  \item $X\sim \pi(\lambda)$，$M_X (t) = \exp(\lambda(e^t-1))$
  \item $X\sim N(\mu,\sigma^2)$，$M_X(t) = \exp(\mu t + \frac{\sigma^2 t^2}{2})$
  \item $X\sim \text{Exp}(\lambda)$，$M_X(t) = \frac{\lambda}{\lambda - t}, t<\lambda$，于是 $\Gamma(n,\lambda)$ 的矩生成函数为 $M_X(t) = \left(\frac{\lambda}{\lambda - t}\right)^n, t<\lambda$，$\chi^2(n)$ 的为 $M_X(t) = (1-2t)^{-\frac n2}, t<1/2$
\end{itemize}

\textbf{常见 Chernoff-Hoeffding 界}：\begin{itemize}
  \item $X\sim \pi(\lambda)$，$P(X\ge x) \le \frac{e^{-\lambda}(e\lambda)^x}{x^x}$
  \item $X\sim N(\mu,\sigma^2)$，$P(X \ge E(X) + k\sigma) \le \exp(-k^2/2)$
  \item $X\sim B(n,p)$，$P(|X-E(X)|\ge n\epsilon)\le 2\cdot \exp(-2n\epsilon^2)$
\end{itemize}

\textbf{对期望分段放缩}：对于 $E(Y) = \sum_{k=1}^n P(Y=k)\cdot k$，若我们知道对于 $k>k'$ 有 $P(Y=k)\le c$，那么就可以分段放缩：$$E(Y) = \sum_{k=1}^{k'} P(Y=k)\cdot k + \sum_{k=k'+1}^n P(Y=k)\cdot k$$ 左边的 $k$ 放成 $k'$，右边的放成 $n$，然后和 $c$ 消掉。

\subsection{大数定律}

\textbf{大数定律的一般形式}：对于随机变量 $\{X_n\}$，对于任意 $\epsilon>0$，若$$\lim_{n\to \infty}P\left(\left|\frac1n\sum_{i=1}^{n}X_i-\frac1n\sum_{i=1}^{n}E(X_i)\right|<\epsilon\right)=1$$

\textbf{Markov 大数定律}：$\frac{1}{n^2}\var{\sum_{i=1}^{n}X_i}\to 0$，则 $\{X_n\}$ 满足大数定律的一般形式。

\textbf{辛钦大数定律}：$\{X_n\}$ 独立同分布，且 $E(X_i)=\mu$，则 $\{X_n\}$ 满足大数定律的一般形式。对比 Markov，需要 iid，但不需要方差。

\textbf{依概率收敛}：随机变量序列 $\{X_n\}$ 依概率收敛于 $X$，记作 $X_n \xrightarrow{P} X$，如果对任意 $\epsilon>0$，有 $$\lim_{n\to \infty} P(|X_n - X| \ge \epsilon) = 0$$

\textbf{依分布收敛}：随机变量序列 $\{X_n\}$ 依分布收敛于 $X$，记作 $X_n \xrightarrow{d} X$，如果对任意 $x$，$F_{X_n}(x) \to F_X(x)$。\textbf{依概率收敛可以推出依分布收敛，反之不亦然}

\textbf{几乎必然收敛}：随机变量序列 $\{X_n\}$ 几乎必然收敛于 $X$，记作 $X_n \xrightarrow{a.s.} X$，如果 $\forall \epsilon>0$ $$\lim_{n\to\infty}P\left(\bigcup_{m=n}^\infty |X_m-X|\ge \epsilon\right) = 1$$

\subsection{应用}

\subsubsection{随机快排}

算法：随机一个 pivot $x$，将其余元素排在两侧 $L,R$，然后递归 $L,R$。如何计算 $E(T)$？$T = O(\sum_{i<j}1_{C_{i,j}})$，$C_{i,j}$ 表示 $i,j$ 是否比较过。

发现算法比较过 $i,j$ iff $i$ 或 $j$ 是 $[i,j]$ 中第一个被选为 pivot 的元素。因为每个元素被选为 pivot 的概率相等，$P(C_{i,j}) = \frac{2}{j-i+1}$。所以 $E(T) = \sum_{i<j} \frac{2}{j-i+1}=O(n\log n)$。

令 $D_i$ 表示 $i$ 被比较的次数，给出尾不等式。发现若 pivot 落在 $[n/4,3n/4]$ 则 $i$ 所在数组大小至少减小 $1/4$，前者概率为 $1/2$。若至少有 $3\log n$ 次，则完成排序。即 $-(3/4)^{3\log n}\le 1/n$。所以 $P(D_i>20\log n)\le P(X_i<3\log n)$，$X_i\sim B(20\log n,1/2)$。$P(X_i<3\log n)\le \exp(-4\log n)\le 1/n^4$。Union Bound 一下 $P(T>20n\log n)\le 1/n^3$。

\subsubsection{JL 降维}

结论：给定 $x_i \in \mathbb{R}^n$，存在线性映射 $F:\mathbb{R}^n \to \mathbb{R}^k$，其中 $m=O(\epsilon^{-2}\log N)$，$\ge1/2$ 概率 $\forall i,j$，$(1-\epsilon)\|x_i - x_j\|_2^2 \le \|F(x_i)-F(x_j)\|_2^2 \le (1+\epsilon)\|x_i - x_j\|_2^2$。$F=\frac{1}{\sqrt{k}}\cdot Ax$，其中 $A$ 的每个元素独立服从 $N(0,1)$。

思路：对所有可能的 $x=x_i-x_j$ 使用引理然后 Union Bound。

\section{参数估计}

估计量：样本的函数，用于估计未知参数。

\textbf{偏差}：$\text{Bias}(\hat{\theta}) = E(\hat{\theta}) - \theta$，$\text{Bias}(\hat \theta)=0$ 称为无偏估计量。若 $\lim_{n\to\infty}E(\hat\theta)=\theta$，称为\textbf{渐近无偏}估计量。

\textbf{均方误差}：$\text{MSE}(\hat{\theta}) = E((\hat{\theta}-\theta)^2) = \var{\hat{\theta}} + (\text{Bias}(\hat{\theta}))^2$。若无偏则 $\text{MSE}(\hat{\theta}) = \var{\hat{\theta}}$。

\textbf{一致估计量}：若估计量 $\hat{\theta}_n\xrightarrow{P} \theta$，则称 $\hat{\theta}_n$ 为参数 $\theta$ 的一致估计量。等价于 $\text{MSE} \to 0$。

$k$ 阶矩：$A_k = \frac1n \sum_{i=1}^n X_i^k$，$A_k$ 是 $\mu_k = E(X^k)$ 的无偏估计量，且一致。$k$ 阶中心矩：$B_k = \frac1n \sum_{i=1}^n (X_i - \overline{X})^k$。$B_2$ 是 $\sigma^2$ 的渐近无偏估计量，且一致，但不是无偏估计量。因为 $E(B_2) = E(X^2) - E(\overline{X}^2)$，而 $E(\overline{X}^2) = (E(X))^2 + \var{\overline{X}}$（平方的期望减期望的平方），然后 $\var{\overline{X}} = \var{X}/n$，所以 $E(B_2) = \frac{n-1}{n}\var{X}$。\textbf{样本方差} $S^2 = \frac1{n-1}\sum_{i=1}^n (X_i - \overline{X})^2$ 是 $\var{X}$ 的无偏估计量。

% // TODO：p19-20 

\textbf{矩法}：用样本矩替换总体矩。方法是不唯一的。对于 $\sigma^2$ 可以用 $S^2$，$B_2$ 甚至 $A_2-\overline{X}^2$。

\textbf{MLE}：最大化似然函数 $L(\theta)=P(\forall i, X_i=x_i)$。\textbf{MLE 的不变性}：若 $\hat\theta$ 是 $\theta$ 的 MLE，则 $g(\hat\theta)$ 是 $g(\theta)$ 的 MLE。

\textbf{区间估计}：设计统计量 $\hat\theta_L(X_1,\cdots,X_n)$ 和 $\hat\theta_U(X_1,\cdots,X_n)$，使得 $P(\hat\theta_L \le \theta \le \hat\theta_U) \ge 1 - \alpha$。

方法：\textbf{枢轴量法}。设计枢轴量 $G$ 使得 $G$ 的分布与未知参数无关，然后选择 $c,d$ 使得 $P(c \le G \le d) = 1 - \alpha$，从而得到不等式 $c \le G(X_1,\cdots,X_n,\theta) \le d$，解出 $\theta$ 的区间估计。

例子：对于 $X\sim N(\mu,\sigma^2)$，设计 $\sigma^2$ 的 $1-\alpha$ 置信区间。考虑 $\frac{(n-1)S^2}{\sigma^2}\sim \chi^2(n-1)$，令 $\chi^2(n-1)$ 分布函数 $F$，取 $c=F^{-1}(\alpha/2), d=F^{-1}(1-\alpha/2)$，则有 $P\left(c \le \frac{(n-1)S^2}{\sigma^2} \le d\right) = 1 - \alpha$，解出 $\sigma^2$ 的区间估计为 $\left[\frac{(n-1)S^2}{d}, \frac{(n-1)S^2}{c}\right]$。对于 $B(1,p)$，可以用 Chernoff $P(|\overline{X}-p|>\epsilon)\le 2e^{-2n\epsilon^2}$，然后让右边等于 $\alpha$ 就可以解出 $\epsilon$。\textbf{注意对于参数 $\theta$ 的区间估计的结果应该是不等号中间是参数 $\theta$}

\subsection{技术}

说明正态总体下 $\overline{X}$ 和 $S^2$ 独立。核心思路：\textbf{通过线性变换将缠在一起的变量 $X_1,\cdots,X_n$（各自包含了均值和方差信息）解耦开来}。构造正交矩阵 $U$，第一行全为 $1/\sqrt{n}$，其他随意。令随机变量 $\boldsymbol{X} = (X_1,\cdots,X_n)$，令 $\boldsymbol{Y} = U\boldsymbol{X}$，显然 $\boldsymbol{Y}$ 服从高斯分布。注意到 $E(\boldsymbol{Y}) = (\sqrt{n}\mu, 0,\cdots, 0)$，且 $\cov{\boldsymbol{Y}} = \sigma^2 I$。因此 $Y_1,\cdots,Y_n$ 独立，且 $Y_1 \sim N(\sqrt{n}\mu, \sigma^2)$，$Y_2,\cdots,Y_n \sim N(0,\sigma^2)$。注意到 $\overline{X} = \frac{Y_1}{\sqrt{n}}$，且 $\sum (X_i - \overline{X})^2 = \sum_{i=2}^n Y_i^2$，因此 $\overline{X}$ 和 $S^2$ 独立。

知道 $\overline{X}\sim N(\mu,\sigma^2/n)$，$(n-1)S^2/\sigma^2 \sim \chi^2(n-1)$（因为是 $n-1$ 个独立的 $N(0,1)$ 的平方和）。

对于 $X\sim\text{Exp}(\lambda)$，计算 $E(1/\overline{X})$ 的时候可以利用 $Y\sim \Gamma(n,\lambda)$，然后可以化出分子和分母的 $\Gamma$ 函数，消掉。

\section{回归分析}

\subsection{一元线性回归}

回归分析：$y=\alpha+\beta x + \epsilon$，其中 $\epsilon$ 为误差项，$E(\epsilon)=0$, $\var{\epsilon}=\sigma^2$。

\textbf{最小二乘}：$Q(\alpha,\beta) = \sum_{i=1}^{n}(y_i-\beta x_i - \alpha)^2$，使得 $Q$ 最小的 $\hat\alpha, \hat\beta$ 称为最小二乘估计。求偏导然后令为 $0$ 可得 $$\hat\beta = \frac{S_{xy}}{S_{xx}}, \hat\alpha = \overline{y} - \hat\beta \overline{x}$$ 其中 $S_{xy} = \sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})$，$S_{xx} = \sum_{i=1}^n (x_i - \overline{x})^2$。

\textbf{一个很关键的技巧}：$\sum (x_i - \overline{x}) = 0$，因此可以对比如 $\sum (x_i-\overline{x})(x_i)$ 的式子进行处理成 $s_{xx}$ 的形式。

\textbf{无偏性}：$\hat{\beta} = \beta + \sum \epsilon_i (x_i - \overline{x})/s_{xx}$，$\hat{\alpha} = \alpha + \sum \epsilon_i \left(\frac 1n - \frac{(x_i - \overline{x})}{s_{xx}}\cdot \overline{x}\right)$

\textbf{估计量的方差与协方差}：$\text{MSE}(\hat{\beta}) = \var{\hat{\beta}} = \sigma^2/s_{xx}$，$\text{MSE}(\hat{\alpha}) = \var{\hat{\alpha}} = \sigma^2 \left(\frac 1n + \frac{(\overline{x})^2}{s_{xx}}\right)$，算协方差的时候同样考虑只有交叉项有贡献，$\cov{\hat{\alpha},\hat{\beta}} = -\sigma^2\cdot \frac{\overline{x}}{s_{xx}}$。

\textbf{预测值的无偏性与方差}：预测值 $\hat{y_i} = \hat{\alpha} + \hat{\beta}x_i$，$E(\hat{y_i}) = \alpha + \beta x_i$，所以无偏。$\var{\hat{y_i}} = \sigma^2\left[\frac 1n + \frac{(x_i - \overline{x})^2}{s_{xx}}\right]$，通过 $\var{\hat{y_i}} = \var{\hat{\alpha}} + x_i^2\var{\hat{\beta}} + 2x_i\cov{\hat{\alpha},\hat{\beta}}$ 计算。

\textbf{残差的方差}：残差 $e_i = y_i - \hat{y_i}$，$E(e_i) = 0$，展开方差的公式来计算 $\var{e_i} = \sigma^2\left[1 - \frac 1n - \frac{(x_i - \overline{x})^2}{s_{xx}}\right]$。

\textbf{$\sigma^2$ 的无偏估计}：$E(\sum (\hat{y}_i - y_i)^2) = \sum\var{\hat{y}_i - y_i} = (n-2)\sigma^2$，所以 $s^2 = \frac{1}{n-2}\sum(\hat{y_i} - y_i)^2$ 为无偏估计量。

\textbf{最大似然}：需要 $\epsilon_i \sim N(0,\sigma^2)$ 且相互独立，对于 $\alpha,\beta$ 等价于最小二乘，但是 $\hat{\sigma^2}_{\text{MLE}} = \frac 1n\sum(y_i - \hat{y}_i)^2$，是有偏的。$\hat{\alpha}$ 和 $\hat{\beta}$ 服从正态分布（方差我们之前计算过）。若 $s^2 = \frac{1}{n-2}\sum (\hat{y}_i - y_i)^2$，则 $\frac{(n-2)s^2}{\sigma^2} \sim \chi^2(n-2)$，且与 $\hat{\alpha},\hat{\beta}$ 独立。

\subsection{多元线性回归}

$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon$ 写成向量的形式，发现 $y_i = \boldsymbol{x_i}^\top \boldsymbol{\beta} + \epsilon_i$,其中 $\boldsymbol{x} = (1,x_{i,1},\cdots, x_{i,k})$。定义 $Q(\boldsymbol{\beta}) = \sum (y_i - \boldsymbol{\beta}^\top\boldsymbol{x}_i )^2$，最小化之，最小二乘估计 $\hat{\boldsymbol{\beta}}$。经验回归函数为 $\hat{\boldsymbol{y}} = \boldsymbol{x}^\top \hat{\boldsymbol{\beta}}$

矩阵形式即为 $\boldsymbol{X} \in \mathbb{R}^{n\times (k+1)}$，这个时候 $Q(\boldsymbol{\beta}) = |\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}|^2$。正规方程为 $\nabla Q = -2\boldsymbol{X}^\top(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}) = 0$，若 $\boldsymbol{X}$ 列满秩，则 $\hat{\boldsymbol{\beta}} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{y}$。

发现 $\hat{\boldsymbol{\beta}} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top (\boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}) = \beta + (\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{\epsilon}$，所以 $E(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}$，无偏。$\cov{\hat{\boldsymbol{\beta}}} = \sigma^2 (\boldsymbol{X}^\top \boldsymbol{X})^{-1}$。

这里接下来处理一元的情况，注意到 $\boldsymbol{X}^\top\boldsymbol{X} = \begin{pmatrix}
  n&\sum x_i\\ \sum x_i & \sum x_i^2
\end{pmatrix}$ 所以行列式为 $n\cdot s_{xx}$，于是 $\cov{\hat{\boldsymbol{\beta}}} = \frac{\sigma^2}{n\cdot s_{xx}}\begin{pmatrix}
  \sum x_i^2 & -n\overline{x}\\ -n\overline{x} & n
\end{pmatrix}$，可以和之前的一元结果对应上。

$\hat{\boldsymbol{y}} = \boldsymbol{X}\hat{\boldsymbol{\beta}} = \boldsymbol{X}(\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top \boldsymbol{y}$，令 $\boldsymbol{H} = \boldsymbol{X}(\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top$。性质：$\text{tr}(\boldsymbol{H}) = \text{tr}(\boldsymbol{X}(\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top) = \text{tr}((\boldsymbol{X}^\top \boldsymbol{X})^{-1}\boldsymbol{X}^\top\boldsymbol{X}) = \text{tr}(\boldsymbol{I}) = k+1$（trace trick），$\boldsymbol{H}^2=\boldsymbol{H}$，$(\boldsymbol{I-H})^2 = \boldsymbol{I-H}$，$\boldsymbol{HX = X}$，$\boldsymbol{H}$ 对称且半正定。其本质是\textbf{投影矩阵}，将任意向量投影到 $\boldsymbol{X}$ 的列空间上，所以 $\boldsymbol{y} - \hat{\boldsymbol{y}}$ 垂直于该列空间。

所以 $\hat{\boldsymbol{y}}  - \boldsymbol{y} = (\boldsymbol{H-I})\boldsymbol{y} = (\boldsymbol{H-I})\boldsymbol{\epsilon}$，于是 $\cov{\hat{\boldsymbol{y}} - \boldsymbol{y}} = \sigma^2(\boldsymbol{H-I})(\boldsymbol{H-I})^\top = \sigma^2(\boldsymbol{I-H})$，$E(|\hat{\boldsymbol{y}} - \boldsymbol{y}|^2) = \sigma^2\text{tr}(\boldsymbol{I-H}) = \sigma^2(n-k-1)$，所以 $\frac{1}{n-k-1}|\hat{\boldsymbol{y}} - \boldsymbol{y}|^2$ 为 $\sigma^2$ 的无偏估计量。对于一元的情况，$k=1$。

$\text{SST}=\sum(y_i - \overline{y})^2$（总平方和），$\text{SSE}=\sum(\hat{y}_i - y_i)^2$（残差平方和），$\text{SSR}=\sum(\hat{y}_i - \overline{y})^2$（回归平方和）。有 $\text{SST} = \text{SSR} + \text{SSE}$。（证明：知道 $\boldsymbol{y} - \hat{\boldsymbol{y}}$ 垂直于 $C(X)$，而显然 $\overline{\boldsymbol{y}}$ 和 $\hat{\boldsymbol{y}}$ 在列空间内，勾股定理）定义 $R^2 = \text{SSR}/\text{SST}$，表示回归模型对总变异的解释比例。有 $R^2 = 1 - \text{SSE}/\text{SST}$ 且 $R^2 \in [0,1]$。

\end{multicols*}

\end{document}
